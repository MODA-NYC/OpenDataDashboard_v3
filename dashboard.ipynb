{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard\n",
    "based from dashboard_v3.py\n",
    "\n",
    "running this script updates the google sheets which updates the dashbaord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUANTITY ####\n",
    "\n",
    "#### Step 1. Get number of rows\n",
    "\n",
    "# pull the data with row counts and the date of its latest update\n",
    "# https://data.cityofnewyork.us/dataset/Daily-Dataset-Facts/gzid-z3nh\n",
    "row_count_updated, row_count_df = credentials.get_socrata_row_count()\n",
    "dfacts = row_count_df[['asset_title',\n",
    "                       'asset_id_4x4',\n",
    "                       'agency',\n",
    "                       'asset_rows']]\\\n",
    "                .drop_duplicates(subset=['asset_id_4x4'])\n",
    "dfacts['asset_rows'] = pd.to_numeric(dfacts.asset_rows)\n",
    "\n",
    "dfacts_agency_df = dfacts.groupby(['agency'])['asset_rows']\\\n",
    "                         .sum()\\\n",
    "                         .reset_index()\\\n",
    "                         .rename(columns={'asset_rows':'numrows'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2. Get dates of the data updates\n",
    "\n",
    "# Asset Inventory (Private Access)\n",
    "# https://data.cityofnewyork.us/dataset/Asset-Inventory/r8cp-r4rc\n",
    "private_df = credentials.call_socrata_api('r8cp-r4rc')\n",
    "\n",
    "# get the dates each of datasets has been updated\n",
    "dates_df = private_df[private_df.u_id.isin(['gzid-z3nh','5tqd-u88y','qj2z-ibhs'])]\\\n",
    "                [['u_id', 'last_update_date_data']]\n",
    "dates_df['last_update_date_data'] = pd.to_datetime(dates_df.last_update_date_data, \n",
    "                                                     errors='coerce')\\\n",
    "                                            .dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "today_df = pd.DataFrame({'u_id':['NA'],\n",
    "                         'last_update_date_data':[date.today().strftime(\"%Y-%m-%d\")],\n",
    "                         'Source':['1. Dashboard']})\n",
    "\n",
    "dates_df.loc[dates_df.u_id=='gzid-z3nh','Source'] = '2. Row Count'\n",
    "dates_df.loc[dates_df.u_id=='5tqd-u88y','Source'] = '3. Published Asset Inventory'\n",
    "dates_df.loc[dates_df.u_id=='qj2z-ibhs','Source'] = '4. Open Plan Tracker'\n",
    "dates_df = dates_df.append(today_df)\n",
    "dates_df.reset_index(inplace=True, drop=True)\n",
    "dates_df = dates_df[['Source', 'last_update_date_data']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 3. Get number of datasets\n",
    "\n",
    "# Local Law 251 of 2017: Published Data Asset Inventory\n",
    "# https://data.cityofnewyork.us/City-Government/Local-Law-251-of-2017-Published-Data-Asset-Invento/5tqd-u88y\n",
    "public_df = credentials.call_socrata_api('5tqd-u88y')\n",
    "\n",
    "public_df = public_df[[\n",
    " 'agency',\n",
    " 'name',\n",
    " 'u_id',\n",
    " 'dataset_link',\n",
    " 'date_made_public',\n",
    " 'automation',\n",
    " 'update_frequency',\n",
    " 'last_update_date_data']]\n",
    "\n",
    "# asset inventory has \"type\" of asset column (published view does not)\n",
    "private_df = private_df[['u_id','type','derived_view']]\n",
    "public_df = public_df.merge(private_df,on='u_id',how='left')\n",
    "\n",
    "# Create merged_filter, the dataframe that has only assets defined as datasets\n",
    "# ZF approved the list\n",
    "dataset_filter_list = ['dataset','filter', 'gis map']\n",
    "public_filtered_df = public_df[public_df.type.isin(dataset_filter_list)]\n",
    "\n",
    "#### Step 4. Create one main dataset-level dataframe\n",
    "\n",
    "# extract URL\n",
    "public_filtered_df['dataset_link'] = public_filtered_df['dataset_link']\\\n",
    "                                            .apply(lambda x: list(x.values())[0])\n",
    "\n",
    "# convert to date\n",
    "# fix one date typo\n",
    "public_filtered_df.loc[public_filtered_df['date_made_public']=='August 9, 2-019',\\\n",
    "                       'date_made_public'] = 'August 9, 2019'\n",
    "\n",
    "public_filtered_df['date_made_public_dt'] = pd.to_datetime(\n",
    "                                            pd.to_datetime(public_filtered_df['date_made_public'],\n",
    "                                                           errors='coerce')\\\n",
    "                                            .dt.strftime('%m/%d/%Y'), format=('%m/%d/%Y'))\n",
    "public_filtered_df['last_update_date_data_dt'] = pd.to_datetime(\n",
    "                                                 pd.to_datetime(public_filtered_df['last_update_date_data'])\\\n",
    "                                                 .dt.strftime('%m/%d/%Y'))\n",
    "\n",
    "public_filtered_df.drop(columns=['date_made_public','last_update_date_data'],inplace=True)\n",
    "\n",
    "# append number of rows\n",
    "quantity_dataset_df = public_filtered_df.merge(dfacts[['asset_id_4x4','asset_rows']], \n",
    "                                           left_on='u_id',\n",
    "                                           right_on='asset_id_4x4',\n",
    "                                           how='left')\n",
    "quantity_dataset_df.rename(columns={'asset_rows':'numrows'}, inplace=True)\n",
    "\n",
    "keep_quant_cols=[\n",
    " 'u_id',\n",
    " 'agency',\n",
    " 'name',\n",
    " 'dataset_link',\n",
    " 'type',\n",
    " 'date_made_public_dt',\n",
    " 'last_update_date_data_dt',\n",
    " 'numrows'\n",
    "]\n",
    "\n",
    "quantity_dataset_df = quantity_dataset_df[keep_quant_cols]\n",
    "\n",
    "#### Step 5. Create one main agency-level dataframe\n",
    "\n",
    "# if agency is missing, create NA category\n",
    "quantity_dataset_df['agency'] = quantity_dataset_df.agency.fillna('NA')\n",
    "quantity_agency_df = quantity_dataset_df.groupby(['agency'])\\\n",
    "                            .agg({'u_id':'size','numrows':'sum'})\\\n",
    "                            .reset_index()\\\n",
    "                            .rename(columns={'u_id':'numdatasets'})\n",
    "\n",
    "#### QUALITY (Data Freshness) ####\n",
    "\n",
    "#### Step 1. Build baseline dataset\n",
    "\n",
    "freshness_df = public_filtered_df[[\n",
    "    'agency',\n",
    "    'name',\n",
    "    'u_id',\n",
    "    'update_frequency',\n",
    "    'dataset_link',\n",
    "    'date_made_public_dt',\n",
    "    'last_update_date_data_dt',\n",
    "    'automation']]\n",
    "\n",
    "# Remove datasets with update frequencies for which we cannot determine freshness\n",
    "freshness_df = freshness_df[(~freshness_df['update_frequency']\\\n",
    "                            .isin(['Historical Data', 'As needed'])) &\\\n",
    "                             ~freshness_df['update_frequency'].isna()]\\\n",
    "                            .reset_index(drop=True)\n",
    "\n",
    "def assign_dataframe_statuses(data):\n",
    "\n",
    "    \"\"\"\n",
    "    Determines if the data has been updated on time\n",
    "    \"\"\"\n",
    "    \n",
    "    df = data.copy()\n",
    "\n",
    "    # some values have spaces\n",
    "    df['update_frequency'] = df.update_frequency.str.strip()\n",
    "    \n",
    "    # assign time by update frequency\n",
    "    status_conditions = [\n",
    "        (df['update_frequency']=='Annually'),\n",
    "        (df['update_frequency']=='Monthly'),\n",
    "        (df['update_frequency']=='Quarterly'),\n",
    "        (df['update_frequency']=='Daily'),\n",
    "        (df['update_frequency']=='Biannually'),\n",
    "        (df['update_frequency']=='Weekly'),\n",
    "        (df['update_frequency']=='Triannually'),\n",
    "        (df['update_frequency']=='Weekdays'),\n",
    "        (df['update_frequency']=='2 to 4 times per year'),\n",
    "        (df['update_frequency']=='Biweekly'),\n",
    "        (df['update_frequency']=='Several times per day'),\n",
    "        (df['update_frequency']=='Hourly'),\n",
    "    ]\n",
    "    status_choices = [\n",
    "        pd.Timedelta('365 days'),\n",
    "        pd.Timedelta('31 days'),\n",
    "        pd.Timedelta('92 days'),\n",
    "        pd.Timedelta('25 hours'),\n",
    "        pd.Timedelta('182 days'),\n",
    "        pd.Timedelta('7 days'),\n",
    "        pd.Timedelta('122 days'),\n",
    "        pd.Timedelta('5 days'),\n",
    "        pd.Timedelta('182 days'),\n",
    "        pd.Timedelta('4 days'),\n",
    "        pd.Timedelta('25 hours'),\n",
    "        pd.Timedelta('25 hours')\n",
    "        ]\n",
    "    \n",
    "    df['update_threshold'] = np.select(status_conditions, status_choices, default=pd.Timedelta('50000 days'))\n",
    "    \n",
    "    # calculate when asset should have been last updated\n",
    "    df['last_updated_ago'] = pd.to_datetime(date.today()) - df.last_update_date_data_dt\n",
    "    \n",
    "    # assign status to automated, dictionary and geocoded columns\n",
    "    df['fresh'] = np.where((df['last_updated_ago']>=df['update_threshold']),'No','Yes')\n",
    "    \n",
    "    df.drop(columns=['update_threshold'],inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "freshness_df = assign_dataframe_statuses(freshness_df)\n",
    "\n",
    "# ensure that datasets with missing agency value are accounted for\n",
    "freshness_df['agency'] = freshness_df.agency.fillna('NA')\n",
    "\n",
    "keep_fresh_cols = [\n",
    " 'u_id',\n",
    " 'agency',\n",
    " 'name',\n",
    " 'dataset_link',\n",
    " 'automation',\n",
    " 'update_frequency',\n",
    " 'last_update_date_data_dt',\n",
    " 'fresh'    \n",
    "]\n",
    "\n",
    "freshness_dataset_df = freshness_df[keep_fresh_cols]\n",
    "\n",
    "#### Step 2. Calculate average data freshness by agency\n",
    "\n",
    "# get the count of fresh dataset by agency\n",
    "fresh_count_df = freshness_df[freshness_df.fresh=='Yes'].groupby(['agency'])\\\n",
    "                                .size()\\\n",
    "                                .reset_index()\\\n",
    "                                .rename(columns={0:'fresh_count'})\n",
    "\n",
    "# get the total count of datasets by agency (excluding historical and as needed)\n",
    "freshness_agency_df = freshness_df.groupby(['agency'])\\\n",
    "                                .size()\\\n",
    "                                .reset_index()\\\n",
    "                                .rename(columns={0:'total_auto_count'})\\\n",
    "                                .merge(fresh_count_df, on='agency',how='left')\n",
    "\n",
    "# calculate percent freshly updated\n",
    "freshness_agency_df['fresh_pct'] = freshness_agency_df.fresh_count / freshness_agency_df.total_auto_count\n",
    "\n",
    "#### COMPLIANCE ####\n",
    "\n",
    "#### Step 1. Build baseline dataset\n",
    "\n",
    "# NYC Open Data Release Tracker\n",
    "# https://data.cityofnewyork.us/City-Government/NYC-Open-Data-Release-Tracker/qj2z-ibhs\n",
    "tracker_df = credentials.call_socrata_api('qj2z-ibhs')\n",
    "\n",
    "# exclude Removed from the plan and Removed from the portal, \n",
    "release_status_filter = [\n",
    "    'Released',\n",
    "    'Scheduled for release',\n",
    "    'Under Review'\n",
    "]\n",
    "tracker_df = tracker_df[tracker_df.release_status.isin(release_status_filter)]\n",
    "\n",
    "# apply grace period for release date\n",
    "grace_period_days = 14\n",
    "today = date.today()\n",
    "\n",
    "tracker_df['original_plan_date_dt'] = pd.to_datetime(tracker_df.original_plan_date)\n",
    "tracker_df['latest_plan_date_dt'] = pd.to_datetime(tracker_df.latest_plan_date)\n",
    "tracker_df['release_date_dt'] = pd.to_datetime(tracker_df.release_date)\n",
    "\n",
    "# number of days between release and planned date\n",
    "tracker_df['plan_to_release'] = (tracker_df.release_date_dt - tracker_df.latest_plan_date_dt).dt.days\n",
    "\n",
    "# create a check if released on time\n",
    "tracker_df['within_grace_period'] = np.where((tracker_df['plan_to_release'] < grace_period_days), 'Yes', 'No')\n",
    "tracker_df['within_grace_period_num'] = tracker_df['plan_to_release'] < grace_period_days\n",
    "\n",
    "# subset datasets that were supposed to be released in the last 12 months\n",
    "tracker_df['last_12_months'] = ((pd.to_datetime(today) - tracker_df.latest_plan_date_dt).dt.days < 365) & \\\n",
    "                                (tracker_df.latest_plan_date_dt <= pd.to_datetime(today))\n",
    "\n",
    "tracker_df['dataset_link'] = tracker_df['url1']\\\n",
    "                                            .apply(lambda x: list(x.values())[0] \\\n",
    "                                                   if type(x) is dict else 'NA')\n",
    "# drop duplicates for released datasets\n",
    "# keep the one with the oldest release date\n",
    "tracker_df = tracker_df[~tracker_df.u_id.isna()]\\\n",
    "                                .sort_values(by='release_date_dt')\\\n",
    "                                .drop_duplicates(subset=['u_id'], keep='first')\\\n",
    "                                .append(tracker_df[tracker_df.u_id.isna()])\n",
    "\n",
    "\n",
    "tracker_12mo_df = tracker_df[tracker_df.last_12_months]\n",
    "\n",
    "tracker_12mo_df['latest_plan_date_dt'] = tracker_12mo_df.latest_plan_date_dt.dt.strftime(\"%Y-%m-%d\")\n",
    "tracker_12mo_df['release_date_dt'] = tracker_12mo_df.release_date_dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#### Step 2. Build dataset-level dataset\n",
    "\n",
    "keep_tracker_cols = [\n",
    " 'u_id',\n",
    " 'agency',\n",
    " 'dataset',\n",
    " 'dataset_description',\n",
    " 'latest_plan_date_dt',\n",
    " 'release_status',\n",
    " 'release_date_dt',\n",
    " 'within_grace_period',\n",
    " 'within_grace_period_num',\n",
    " 'dataset_link'\n",
    "]\n",
    "\n",
    "tracker_12mo_dataset_df = tracker_12mo_df[keep_tracker_cols]\n",
    "\n",
    "# drop duplicates for released datasets\n",
    "# keep the one with the oldest release date\n",
    "# tracker_12mo_dataset_clean_df = tracker_12mo_dataset_df[~tracker_12mo_dataset_df.u_id.isna()]\\\n",
    "#                                 .sort_values(by='release_date_dt')\\\n",
    "#                                 .drop_duplicates(subset=['u_id'], keep='first')\\\n",
    "#                                 .append(tracker_12mo_dataset_df[tracker_12mo_dataset_df.u_id.isna()])\n",
    "\n",
    "# append type and agency from public inventory\n",
    "tracker_12mo_dataset_df = tracker_12mo_dataset_df.merge(public_df[['u_id','type','agency']], \n",
    "                                                                    on='u_id',\n",
    "                                                                    how='left')\n",
    "\n",
    "# update agency name to match public inventory (can only be done for already published datasets)\n",
    "tracker_12mo_dataset_df['agency'] = np.where((tracker_12mo_dataset_df.release_status=='Released') & \\\n",
    "                                                   ~tracker_12mo_dataset_df.agency_y.isna(),\n",
    "                                                  tracker_12mo_dataset_df.agency_y,\n",
    "                                                  tracker_12mo_dataset_df.agency_x)\n",
    "tracker_12mo_dataset_df.drop(columns=['agency_x', 'agency_y'], inplace=True)\n",
    "\n",
    "#### Step 3. Build agency-level dataset\n",
    "tracker_12mo_agency_df = tracker_12mo_dataset_df.groupby(['agency'])\\\n",
    "                                        .agg({'agency':'size',\n",
    "                                              'within_grace_period_num':'sum'})\\\n",
    "                                        .rename(columns={'agency':'tracker_dataset_count',\n",
    "                                                         'within_grace_period_num':'tracker_count_ontime'})\\\n",
    "                                        .reset_index()\n",
    "\n",
    "# calculate percent released on time\n",
    "tracker_12mo_agency_df['pct_ontime'] = tracker_12mo_agency_df.tracker_count_ontime/tracker_12mo_agency_df.tracker_dataset_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### DASHBOARD ####\n",
    "\n",
    "#### Step 1. Get citywide metrics\n",
    "\n",
    "# total number of rows\n",
    "cw_numrows = quantity_agency_df.numrows.sum()\n",
    "\n",
    "# total number of datasets\n",
    "cw_numdatasets = quantity_agency_df.numdatasets.sum()\n",
    "# percent updated on time\n",
    "cw_freshness = freshness_dataset_df[freshness_dataset_df.fresh=='Yes'].shape[0]/\\\n",
    "                    freshness_df.shape[0]\n",
    "# percent released on time\n",
    "cw_compliance = tracker_12mo_dataset_df.within_grace_period_num.sum()/ \\\n",
    "                tracker_12mo_dataset_df.shape[0]\n",
    "\n",
    "citywide = pd.DataFrame([['Citywide',\n",
    "                         cw_numrows,\n",
    "                         cw_numdatasets,\n",
    "                         cw_freshness,\n",
    "                         cw_compliance]],\n",
    "                       columns=['Scope',\n",
    "                                'Number of published rows',\n",
    "                                'Number of published datasets',\n",
    "                                'Percent of datasets updated on time',\n",
    "                                'Percent of datasets released on time in the last 12 months'])\n",
    "\n",
    "citywide = citywide.fillna('NA')\n",
    "\n",
    "#### Step 2. Build complete agency-level dataset\n",
    "\n",
    "all_agency_df = quantity_agency_df.merge(freshness_agency_df, \n",
    "                                        on='agency',\n",
    "                                        how='outer')\\\n",
    "                                  .merge(tracker_12mo_agency_df, \n",
    "                                        on='agency',\n",
    "                                        how='outer')\n",
    "\n",
    "all_agency_df = all_agency_df.fillna('NA')\n",
    "\n",
    "#### Step 3. Build complete dataset-level dataset\n",
    "\n",
    "# aggregate freshness data and tracker data (for released datasets only)\n",
    "all_datasets_df = quantity_dataset_df.merge(freshness_dataset_df.drop(columns=\n",
    "                                                                      ['agency',\n",
    "                                                                      'name',\n",
    "                                                                      'dataset_link',\n",
    "                                                                      'last_update_date_data_dt']), \n",
    "                                        on='u_id',\n",
    "                                        how='outer')\\\n",
    "                                  .merge(tracker_12mo_dataset_df.drop(columns=\n",
    "                                                                            ['agency',\n",
    "                                                                             'dataset',\n",
    "                                                                             'type',\n",
    "                                                                             'dataset_link']), \n",
    "                                        on='u_id',\n",
    "                                        how='left')\n",
    "\n",
    "# append non-released datasets data\n",
    "# doing it as a separate step to keep more accurate data for released datasets\n",
    "all_datasets_df = all_datasets_df.append(tracker_12mo_dataset_df[~tracker_12mo_dataset_df.u_id.isin(all_datasets_df.u_id)])\\\n",
    "                                 .reset_index(drop=True)\n",
    "\n",
    "# merge name and dataset columns since they contain the same information\n",
    "all_datasets_df.loc[all_datasets_df.name.isna(),'name'] = all_datasets_df.dataset\n",
    "\n",
    "# merge automation/update data for \"historical\" and \"as needed\" datasets\n",
    "all_datasets_df = all_datasets_df.merge(public_df[['u_id','automation','update_frequency']], on='u_id', how='left')\n",
    "all_datasets_df['automation'] = all_datasets_df.automation_x\n",
    "all_datasets_df['update_frequency'] = all_datasets_df.update_frequency_x\n",
    "all_datasets_df.loc[all_datasets_df.automation_x.isna(),'automation'] = all_datasets_df.automation_y\n",
    "all_datasets_df.loc[all_datasets_df.update_frequency_x.isna(),'update_frequency'] = all_datasets_df.update_frequency_y\n",
    "\n",
    "# recode missing dates into string NA to properly read format in GDS\n",
    "all_datasets_df['release_date_dt_fix'] = pd.to_datetime(all_datasets_df.release_date_dt, errors='coerce')\n",
    "\n",
    "all_datasets_df = all_datasets_df[[\n",
    " 'agency',\n",
    " 'u_id',\n",
    " 'name',\n",
    " 'dataset_description',\n",
    " 'dataset_link',\n",
    " 'type',\n",
    " 'date_made_public_dt',\n",
    " 'numrows',\n",
    " 'automation',\n",
    " 'update_frequency',\n",
    " 'last_update_date_data_dt',\n",
    " 'fresh',\n",
    " 'latest_plan_date_dt',\n",
    " 'release_status',\n",
    " 'release_date_dt_fix',\n",
    " 'within_grace_period']]\n",
    "\n",
    "all_datasets_df = all_datasets_df.fillna('NA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid credentials supplied. Will generate from default token.\n",
      "\n",
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=113111434985-lvo5fc9k31asajq93cm6gkpdneq6n7n0.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.metadata.readonly+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive+https%3A%2F%2Fspreadsheets.google.com%2Ffeeds+https%3A%2F%2Fdocs.google.com%2Ffeeds&access_type=offline&response_type=code\n",
      "\n",
      "If your browser is on a different machine then exit and re-run this\n",
      "application with the command-line parameter\n",
      "\n",
      "  --noauth_local_webserver\n",
      "\n",
      "Authentication successful.\n",
      "Upload complete for citywide dataset\n",
      "Invalid credentials supplied. Will generate from default token.\n",
      "Upload complete for agency dataset\n",
      "Invalid credentials supplied. Will generate from default token.\n",
      "Upload complete for datasets dataset\n",
      "Invalid credentials supplied. Will generate from default token.\n",
      "Upload complete for dates dataset\n"
     ]
    }
   ],
   "source": [
    "#### Step 4. Upload data to Google Spreadsheets\n",
    "\n",
    "credentials.gs_upload(df=citywide, \n",
    "                    wks_name='_citywide_')\n",
    "print('Upload complete for citywide dataset')\n",
    "\n",
    "credentials.gs_upload(df=all_agency_df, \n",
    "                    wks_name='_agency_')\n",
    "print('Upload complete for agency dataset')\n",
    "\n",
    "credentials.gs_upload(df=all_datasets_df, \n",
    "                    wks_name='_datasets_')\n",
    "print('Upload complete for datasets dataset')\n",
    "\n",
    "credentials.gs_upload(df=dates_df, \n",
    "                    wks_name='_dates_')\n",
    "print('Upload complete for dates dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
