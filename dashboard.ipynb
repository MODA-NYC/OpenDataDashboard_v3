{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Data Dashboard\n",
    "\n",
    "QA script for dashboard_v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete in production code\n",
    "# from importlib import reload\n",
    "# reload(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset facts dataset was last updated on: 2020-11-18T00:00:00.000\n"
     ]
    }
   ],
   "source": [
    "#### QUANTITY ####\n",
    "\n",
    "#### Step 1. Get number of rows\n",
    "\n",
    "# pull the data with row counts and the date of its latest update\n",
    "# https://data.cityofnewyork.us/dataset/Daily-Dataset-Facts/gzid-z3nh\n",
    "row_count_updated, row_count_df = credentials.get_socrata_row_count()\n",
    "dfacts = row_count_df[['asset_title',\n",
    "                       'asset_id_4x4',\n",
    "                       'agency',\n",
    "                       'asset_rows']]\\\n",
    "                .drop_duplicates(subset=['asset_id_4x4'])\n",
    "dfacts['asset_rows'] = pd.to_numeric(dfacts.asset_rows)\n",
    "\n",
    "dfacts_agency_df = dfacts.groupby(['agency'])['asset_rows']\\\n",
    "                         .sum()\\\n",
    "                         .reset_index()\\\n",
    "                         .rename(columns={'asset_rows':'numrows'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n/a         6292\n",
       "filter       910\n",
       "chart        644\n",
       "table        308\n",
       "map          236\n",
       "map view     144\n",
       "datalens     116\n",
       "calendar      10\n",
       "Name: derived_asset_type, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_count_df['derived_asset_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2. Get dates of the data updates\n",
    "\n",
    "# Asset Inventory (Private Access)\n",
    "# https://data.cityofnewyork.us/dataset/Asset-Inventory/kvci-ugf9\n",
    "private_df = credentials.call_socrata_api('kvci-ugf9')\n",
    "# old asset inventory is r8cp-r4rc\n",
    "\n",
    "# get the dates each of datasets has been updated\n",
    "dates_df = private_df[private_df.uid.isin(['gzid-z3nh','5tqd-u88y','qj2z-ibhs'])]\\\n",
    "                [['uid', 'last_data_updated_date']]\n",
    "\n",
    "## if old asset inventory is used:\n",
    "# dates_df = private_df[private_df.u_id.isin(['gzid-z3nh','5tqd-u88y','qj2z-ibhs'])]\\\n",
    "#                 [['u_id', 'last_update_date_data']]\n",
    "\n",
    "dates_df['last_data_updated_date'] = pd.to_datetime(dates_df.last_data_updated_date, \n",
    "                                                     errors='coerce')\\\n",
    "                                            .dt.strftime(\"%Y-%m-%d\")\n",
    "# dates_df['last_update_date_data'] = pd.to_datetime(dates_df.last_update_date_data, \n",
    "#                                                      errors='coerce')\\\n",
    "#                                             .dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "today_df = pd.DataFrame({'uid':['NA'],\n",
    "                         'last_data_updated_date':[date.today().strftime(\"%Y-%m-%d\")],\n",
    "                         'Source':['1. Dashboard']})\n",
    "# today_df = pd.DataFrame({'u_id':['NA'],\n",
    "#                          'last_update_date_data':[date.today().strftime(\"%Y-%m-%d\")],\n",
    "#                          'Source':['1. Dashboard']})\n",
    "\n",
    "dates_df.loc[dates_df.uid=='gzid-z3nh','Source'] = '2. Row Count'\n",
    "dates_df.loc[dates_df.uid=='5tqd-u88y','Source'] = '3. Published Asset Inventory'\n",
    "dates_df.loc[dates_df.uid=='qj2z-ibhs','Source'] = '4. Open Plan Tracker'\n",
    "dates_df = dates_df.append(today_df)\n",
    "dates_df.reset_index(inplace=True, drop=True)\n",
    "dates_df = dates_df[['Source', 'last_data_updated_date']]\n",
    "dates_df.rename(columns={'last_data_updated_date':'updated_on'},inplace=True)\n",
    "# dates_df = dates_df[['Source', 'last_update_date_data']]\n",
    "# dates_df.rename(columns={'last_update_date_data':'updated_on'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasetinformation_agency',\n",
       " 'name',\n",
       " 'description',\n",
       " 'type',\n",
       " 'category',\n",
       " 'legislativecompliance_datasetfromtheopendataplan',\n",
       " 'url',\n",
       " 'uid',\n",
       " 'update_datemadepublic',\n",
       " 'update_updatefrequency',\n",
       " 'last_data_updated_date',\n",
       " 'legislativecompliance_candatasetfeasiblybeautomated',\n",
       " 'update_automation',\n",
       " 'legislativecompliance_hasdatadictionary',\n",
       " 'legislativecompliance_containsaddress',\n",
       " 'legislativecompliance_geocoded',\n",
       " 'legislativecompliance_existsexternallyll1102015',\n",
       " 'legislativecompliance_externalfrequencyll1102015',\n",
       " 'legislativecompliance_removedrecords',\n",
       " 'visits',\n",
       " 'row_count',\n",
       " 'column_count']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_df = credentials.call_socrata_api('5tqd-u88y')\n",
    "list(public_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['last_update_date_data', 'update_frequency', 'u_id', 'agency', 'automation', 'dataset_link', 'date_made_public'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-73a53912b7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m  \u001b[0;34m'automation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m  \u001b[0;34m'update_frequency'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m  'last_update_date_data']]\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# asset inventory has \"type\" of asset column (published view does not)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['last_update_date_data', 'update_frequency', 'u_id', 'agency', 'automation', 'dataset_link', 'date_made_public'] not in index\""
     ]
    }
   ],
   "source": [
    "#### Step 3. Get number of datasets\n",
    "\n",
    "# Local Law 251 of 2017: Published Data Asset Inventory\n",
    "# https://data.cityofnewyork.us/City-Government/Local-Law-251-of-2017-Published-Data-Asset-Invento/5tqd-u88y\n",
    "public_df = credentials.call_socrata_api('5tqd-u88y')\n",
    "\n",
    "public_df = public_df[[\n",
    " 'agency',\n",
    " 'name',\n",
    " 'u_id',\n",
    " 'dataset_link',\n",
    " 'date_made_public',\n",
    " 'automation',\n",
    " 'update_frequency',\n",
    " 'last_update_date_data']]\n",
    "\n",
    "# asset inventory has \"type\" of asset column (published view does not)\n",
    "private_df = private_df[['uid','type','audience','derived_view','parent_uid']]\n",
    "public_df = public_df.merge(private_df,\n",
    "                            left_on='u_id',\n",
    "                            right_on='uid',\n",
    "                            how='left')\n",
    "\n",
    "# Create merged_filter, the dataframe that has only assets defined as datasets\n",
    "# ZF approved the list\n",
    "dataset_filter_list = ['dataset','filter','gis map','map']\n",
    "public_filtered_df = public_df[public_df.type.isin(dataset_filter_list)]\n",
    "\n",
    "parent_uids = public_filtered_df[public_filtered_df['derived_view']==True]['parent_uid']\n",
    "exc_parent_uids = private_df[private_df['uid'].isin(parent_uids) & (private_df['audience']=='public')]['uid']\n",
    "public_filtered_df = public_filtered_df[~public_filtered_df['parent_uid'].isin(exc_parent_uids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 4. Create one main dataset-level dataframe\n",
    "\n",
    "# extract URL\n",
    "public_filtered_df['dataset_link'] = public_filtered_df['dataset_link']\\\n",
    "                                            .apply(lambda x: list(x.values())[0])\n",
    "\n",
    "# convert to date\n",
    "# fix one date typo\n",
    "public_filtered_df.loc[public_filtered_df['date_made_public']=='August 9, 2-019',\\\n",
    "                       'date_made_public'] = 'August 9, 2019'\n",
    "\n",
    "public_filtered_df['date_made_public_dt'] = pd.to_datetime(\n",
    "                                            pd.to_datetime(public_filtered_df['date_made_public'],\n",
    "                                                           errors='coerce')\\\n",
    "                                            .dt.strftime('%m/%d/%Y'), format=('%m/%d/%Y'))\n",
    "public_filtered_df['last_update_date_data_dt'] = pd.to_datetime(\n",
    "                                                 pd.to_datetime(public_filtered_df['last_update_date_data'])\\\n",
    "                                                 .dt.strftime('%m/%d/%Y'))\n",
    "\n",
    "public_filtered_df.drop(columns=['date_made_public','last_update_date_data'],inplace=True)\n",
    "\n",
    "# append number of rows\n",
    "quantity_dataset_df = public_filtered_df.merge(dfacts[['asset_id_4x4','asset_rows']], \n",
    "                                           left_on='u_id',\n",
    "                                           right_on='asset_id_4x4',\n",
    "                                           how='left')\n",
    "quantity_dataset_df.rename(columns={'asset_rows':'numrows'}, inplace=True)\n",
    "\n",
    "keep_quant_cols=[\n",
    " 'u_id',\n",
    " 'agency',\n",
    " 'name',\n",
    " 'dataset_link',\n",
    " 'type',\n",
    " 'date_made_public_dt',\n",
    " 'last_update_date_data_dt',\n",
    " 'numrows'\n",
    "]\n",
    "\n",
    "quantity_dataset_df = quantity_dataset_df[keep_quant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 5. Create one main agency-level dataframe\n",
    "\n",
    "# if agency is missing, create NA category\n",
    "quantity_dataset_df['agency'] = quantity_dataset_df.agency.fillna('Not filled out')\n",
    "quantity_agency_df = quantity_dataset_df.groupby(['agency'])\\\n",
    "                            .agg({'u_id':'size','numrows':'sum'})\\\n",
    "                            .reset_index()\\\n",
    "                            .rename(columns={'u_id':'numdatasets'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUALITY (Data Freshness) ####\n",
    "\n",
    "#### Step 1. Build baseline dataset\n",
    "\n",
    "freshness_df = public_filtered_df[[\n",
    "    'agency',\n",
    "    'name',\n",
    "    'u_id',\n",
    "    'update_frequency',\n",
    "    'dataset_link',\n",
    "    'date_made_public_dt',\n",
    "    'last_update_date_data_dt',\n",
    "    'automation']]\n",
    "\n",
    "# Remove datasets with update frequencies for which we cannot determine freshness\n",
    "freshness_df = freshness_df[(~freshness_df['update_frequency']\\\n",
    "                            .isin(['Historical Data', 'As needed'])) &\\\n",
    "                             ~freshness_df['update_frequency'].isna()]\\\n",
    "                            .reset_index(drop=True)\n",
    "\n",
    "def assign_dataframe_statuses(data):\n",
    "\n",
    "    \"\"\"\n",
    "    Determines if the data has been updated on time\n",
    "    \"\"\"\n",
    "    \n",
    "    df = data.copy()\n",
    "\n",
    "    # some values have spaces\n",
    "    df['update_frequency'] = df.update_frequency.str.strip()\n",
    "    \n",
    "    # assign time by update frequency\n",
    "    status_conditions = [\n",
    "        (df['update_frequency']=='Annually'),\n",
    "        (df['update_frequency']=='Monthly'),\n",
    "        (df['update_frequency']=='Quarterly'),\n",
    "        (df['update_frequency']=='Daily'),\n",
    "        (df['update_frequency']=='Biannually'),\n",
    "        (df['update_frequency']=='Weekly'),\n",
    "        (df['update_frequency']=='Triannually'),\n",
    "        (df['update_frequency']=='Weekdays'),\n",
    "        (df['update_frequency']=='2 to 4 times per year'),\n",
    "        (df['update_frequency']=='Biweekly'),\n",
    "        (df['update_frequency']=='Several times per day'),\n",
    "        (df['update_frequency']=='Hourly'),\n",
    "        (df['update_frequency']=='Every four years')\n",
    "    ]\n",
    "    status_choices = [\n",
    "        pd.Timedelta('365 days'),\n",
    "        pd.Timedelta('31 days'),\n",
    "        pd.Timedelta('92 days'),\n",
    "        pd.Timedelta('25 hours'),\n",
    "        pd.Timedelta('182 days'),\n",
    "        pd.Timedelta('7 days'),\n",
    "        pd.Timedelta('122 days'),\n",
    "        pd.Timedelta('5 days'),\n",
    "        pd.Timedelta('182 days'),\n",
    "        pd.Timedelta('4 days'),\n",
    "        pd.Timedelta('25 hours'),\n",
    "        pd.Timedelta('25 hours'),\n",
    "        pd.Timedelta('1460 days')\n",
    "        ]\n",
    "    \n",
    "    df['update_threshold'] = np.select(status_conditions, status_choices, default=pd.Timedelta('50000 days'))\n",
    "    \n",
    "    # calculate when asset should have been last updated\n",
    "    df['last_updated_ago'] = pd.to_datetime(date.today()) - df.last_update_date_data_dt\n",
    "    \n",
    "    # assign status to automated, dictionary and geocoded columns\n",
    "    df['fresh'] = np.where((df['last_updated_ago']>=df['update_threshold']),'No','Yes')\n",
    "    \n",
    "    df.drop(columns=['update_threshold'],inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "freshness_df = assign_dataframe_statuses(freshness_df)\n",
    "\n",
    "# ensure that datasets with missing agency value are accounted for\n",
    "freshness_df['agency'] = freshness_df.agency.fillna('Not filled out')\n",
    "\n",
    "keep_fresh_cols = [\n",
    " 'u_id',\n",
    " 'agency',\n",
    " 'name',\n",
    " 'dataset_link',\n",
    " 'automation',\n",
    " 'update_frequency',\n",
    " 'last_update_date_data_dt',\n",
    " 'fresh'    \n",
    "]\n",
    "\n",
    "freshness_dataset_df = freshness_df[keep_fresh_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_values_used = {\n",
    "    'Annually',\n",
    "    'Monthly',\n",
    "    'Quarterly',\n",
    "    'Daily',\n",
    "    'Biannually',\n",
    "    'Weekly',\n",
    "    'Triannually',\n",
    "    'Weekdays',\n",
    "    '2 to 4 times per year',\n",
    "    'Biweekly',\n",
    "    'Several times per day',\n",
    "    'Hourly'\n",
    "}\n",
    "\n",
    "update_values_available = set(freshness_dataset_df['update_frequency'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Every four years'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify new update frequency values\n",
    "update_values_available.difference(update_values_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_id</th>\n",
       "      <th>agency</th>\n",
       "      <th>name</th>\n",
       "      <th>dataset_link</th>\n",
       "      <th>automation</th>\n",
       "      <th>update_frequency</th>\n",
       "      <th>last_update_date_data_dt</th>\n",
       "      <th>fresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>vww9-qguh</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "      <td>2013-2019 Attendance Results - School</td>\n",
       "      <td>https://data.cityofnewyork.us/d/vww9-qguh</td>\n",
       "      <td>No</td>\n",
       "      <td>Every four years</td>\n",
       "      <td>2020-02-13</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>6yc2-gsz6</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "      <td>2013-2019 Attendance Results - District</td>\n",
       "      <td>https://data.cityofnewyork.us/d/6yc2-gsz6</td>\n",
       "      <td>No</td>\n",
       "      <td>Every four years</td>\n",
       "      <td>2020-02-12</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>viin-czfn</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "      <td>2013-2019 Attendance Results - Borough</td>\n",
       "      <td>https://data.cityofnewyork.us/d/viin-czfn</td>\n",
       "      <td>No</td>\n",
       "      <td>Every four years</td>\n",
       "      <td>2020-02-12</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>eqhe-kbrh</td>\n",
       "      <td>Department of Education (DOE)</td>\n",
       "      <td>2013-2019 Attendance Results - Citywide</td>\n",
       "      <td>https://data.cityofnewyork.us/d/eqhe-kbrh</td>\n",
       "      <td>No</td>\n",
       "      <td>Every four years</td>\n",
       "      <td>2020-02-12</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          u_id                         agency  \\\n",
       "203  vww9-qguh  Department of Education (DOE)   \n",
       "204  6yc2-gsz6  Department of Education (DOE)   \n",
       "205  viin-czfn  Department of Education (DOE)   \n",
       "206  eqhe-kbrh  Department of Education (DOE)   \n",
       "\n",
       "                                        name  \\\n",
       "203    2013-2019 Attendance Results - School   \n",
       "204  2013-2019 Attendance Results - District   \n",
       "205   2013-2019 Attendance Results - Borough   \n",
       "206  2013-2019 Attendance Results - Citywide   \n",
       "\n",
       "                                  dataset_link automation  update_frequency  \\\n",
       "203  https://data.cityofnewyork.us/d/vww9-qguh         No  Every four years   \n",
       "204  https://data.cityofnewyork.us/d/6yc2-gsz6         No  Every four years   \n",
       "205  https://data.cityofnewyork.us/d/viin-czfn         No  Every four years   \n",
       "206  https://data.cityofnewyork.us/d/eqhe-kbrh         No  Every four years   \n",
       "\n",
       "    last_update_date_data_dt fresh  \n",
       "203               2020-02-13   Yes  \n",
       "204               2020-02-12   Yes  \n",
       "205               2020-02-12   Yes  \n",
       "206               2020-02-12   Yes  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freshness_dataset_df[freshness_dataset_df['update_frequency']=='Every four years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2. Calculate average data freshness by agency\n",
    "\n",
    "# get the count of fresh dataset by agency\n",
    "fresh_count_df = freshness_df[freshness_df.fresh=='Yes'].groupby(['agency'])\\\n",
    "                                .size()\\\n",
    "                                .reset_index()\\\n",
    "                                .rename(columns={0:'fresh_count'})\n",
    "\n",
    "# get the total count of datasets by agency (excluding historical and as needed)\n",
    "freshness_agency_df = freshness_df.groupby(['agency'])\\\n",
    "                                .size()\\\n",
    "                                .reset_index()\\\n",
    "                                .rename(columns={0:'total_auto_count'})\\\n",
    "                                .merge(fresh_count_df, on='agency',how='left')\n",
    "\n",
    "# calculate percent freshly updated\n",
    "freshness_agency_df['fresh_pct'] = freshness_agency_df.fresh_count.fillna(0) / freshness_agency_df.total_auto_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMPLIANCE ####\n",
    "\n",
    "#### Step 1. Build baseline dataset\n",
    "\n",
    "# NYC Open Data Release Tracker\n",
    "# https://data.cityofnewyork.us/City-Government/NYC-Open-Data-Release-Tracker/qj2z-ibhs\n",
    "tracker_df = credentials.call_socrata_api('qj2z-ibhs')\n",
    "\n",
    "# exclude Removed from the plan and Removed from the portal, \n",
    "release_status_filter = [\n",
    "    'Released',\n",
    "    'Scheduled for release',\n",
    "    'Under Review'\n",
    "]\n",
    "tracker_df = tracker_df[tracker_df.release_status.isin(release_status_filter)]\n",
    "\n",
    "# apply grace period for release date\n",
    "grace_period_days = 14\n",
    "today = date.today()\n",
    "\n",
    "tracker_df['original_plan_date_dt'] = pd.to_datetime(tracker_df.original_plan_date)\n",
    "tracker_df['latest_plan_date_dt'] = pd.to_datetime(tracker_df.latest_plan_date)\n",
    "tracker_df['release_date_dt'] = pd.to_datetime(tracker_df.release_date)\n",
    "\n",
    "# number of days between release and planned date\n",
    "tracker_df['plan_to_release'] = (tracker_df.release_date_dt - tracker_df.latest_plan_date_dt).dt.days\n",
    "\n",
    "# create a check if released on time\n",
    "tracker_df['within_grace_period'] = np.where((tracker_df['plan_to_release'] < grace_period_days), 'Yes', 'No')\n",
    "tracker_df['within_grace_period_num'] = tracker_df['plan_to_release'] < grace_period_days\n",
    "\n",
    "# subset datasets that were supposed to be released in the last 12 months\n",
    "tracker_df['last_12_months'] = ((pd.to_datetime(today) - tracker_df.latest_plan_date_dt).dt.days < 365) & \\\n",
    "                                (tracker_df.latest_plan_date_dt <= pd.to_datetime(today))\n",
    "\n",
    "tracker_df['dataset_link'] = tracker_df['url1']\\\n",
    "                                            .apply(lambda x: list(x.values())[0] \\\n",
    "                                                   if type(x) is dict else 'NA')\n",
    "# drop duplicates for released datasets\n",
    "# keep the one with the oldest release date\n",
    "tracker_df = tracker_df[~tracker_df.u_id.isna()]\\\n",
    "                                .sort_values(by='release_date_dt')\\\n",
    "                                .drop_duplicates(subset=['u_id'], keep='first')\\\n",
    "                                .append(tracker_df[tracker_df.u_id.isna()])\n",
    "\n",
    "\n",
    "tracker_12mo_df = tracker_df[tracker_df.last_12_months]\n",
    "\n",
    "tracker_12mo_df['latest_plan_date_dt'] = tracker_12mo_df.latest_plan_date_dt.dt.strftime(\"%Y-%m-%d\")\n",
    "tracker_12mo_df['release_date_dt'] = tracker_12mo_df.release_date_dt.dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Step 2. Build dataset-level dataset\n",
    "\n",
    "keep_tracker_cols = [\n",
    " 'u_id',\n",
    " 'agency',\n",
    " 'dataset',\n",
    " 'dataset_description',\n",
    " 'latest_plan_date_dt',\n",
    " 'release_status',\n",
    " 'release_date_dt',\n",
    " 'within_grace_period',\n",
    " 'within_grace_period_num',\n",
    " 'dataset_link'\n",
    "]\n",
    "\n",
    "tracker_12mo_dataset_df = tracker_12mo_df[keep_tracker_cols]\n",
    "\n",
    "# drop duplicates for released datasets\n",
    "# keep the one with the oldest release date\n",
    "# tracker_12mo_dataset_clean_df = tracker_12mo_dataset_df[~tracker_12mo_dataset_df.u_id.isna()]\\\n",
    "#                                 .sort_values(by='release_date_dt')\\\n",
    "#                                 .drop_duplicates(subset=['u_id'], keep='first')\\\n",
    "#                                 .append(tracker_12mo_dataset_df[tracker_12mo_dataset_df.u_id.isna()])\n",
    "\n",
    "# append type and agency from public inventory\n",
    "tracker_12mo_dataset_df = tracker_12mo_dataset_df.merge(public_df[['u_id','type','agency']], \n",
    "                                                                    on='u_id',\n",
    "                                                                    how='left')\n",
    "\n",
    "# update agency name to match public inventory (can only be done for already published datasets)\n",
    "tracker_12mo_dataset_df['agency'] = np.where((tracker_12mo_dataset_df.release_status=='Released') & \\\n",
    "                                                   ~tracker_12mo_dataset_df.agency_y.isna(),\n",
    "                                                  tracker_12mo_dataset_df.agency_y,\n",
    "                                                  tracker_12mo_dataset_df.agency_x)\n",
    "tracker_12mo_dataset_df.drop(columns=['agency_x', 'agency_y'], inplace=True)\n",
    "\n",
    "# exclude assets that are not datasets, filters and gis maps\n",
    "# keeps assets scheduled for release with type NA\n",
    "\n",
    "tracker_12mo_dataset_df = tracker_12mo_dataset_df[tracker_12mo_dataset_df['u_id'].isin(quantity_dataset_df['u_id']) | \\\n",
    "                                                  (tracker_12mo_dataset_df['release_status']=='Scheduled for release')]\n",
    "\n",
    "# tracker_12mo_dataset_df = tracker_12mo_dataset_df[tracker_12mo_dataset_df['type'].isin(dataset_filter_list) | \\\n",
    "#                                                   (tracker_12mo_dataset_df['release_status']=='Scheduled for release')]\n",
    "\n",
    "#### Step 3. Build agency-level dataset\n",
    "\n",
    "# count number of overdue for release datasets\n",
    "agency_overdue_df = tracker_12mo_dataset_df[tracker_12mo_dataset_df['release_status']=='Scheduled for release'].groupby(['agency']).size().reset_index()\n",
    "agency_overdue_df.rename(columns={0:'overdue_datasets'},inplace=True)\n",
    "\n",
    "tracker_12mo_agency_df = tracker_12mo_dataset_df.groupby(['agency'])\\\n",
    "                                        .agg({'agency':'size',\n",
    "                                              'within_grace_period_num':'sum'})\\\n",
    "                                        .rename(columns={'agency':'tracker_dataset_count',\n",
    "                                                         'within_grace_period_num':'tracker_count_ontime'})\\\n",
    "                                        .merge(agency_overdue_df, on='agency', how='left')\\\n",
    "                                        .reset_index(drop=True)\\\n",
    "                                        .fillna(0)\n",
    "\n",
    "# calculate percent released on time\n",
    "tracker_12mo_agency_df['pct_ontime'] = tracker_12mo_agency_df.tracker_count_ontime.fillna(0)/tracker_12mo_agency_df.tracker_dataset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DASHBOARD ####\n",
    "\n",
    "#### Step 1. Get citywide metrics\n",
    "\n",
    "# total number of rows\n",
    "cw_numrows = quantity_agency_df.numrows.sum()\n",
    "\n",
    "# total number of datasets\n",
    "cw_numdatasets = quantity_agency_df.numdatasets.sum()\n",
    "# percent updated on time\n",
    "cw_freshness = freshness_dataset_df[freshness_dataset_df.fresh=='Yes'].shape[0]/\\\n",
    "                    freshness_df.shape[0]\n",
    "# percent released on time\n",
    "cw_compliance = tracker_12mo_dataset_df.within_grace_period_num.sum()/ \\\n",
    "                tracker_12mo_dataset_df.shape[0]\n",
    "\n",
    "# number of assets that were supposed to be released but were not as of today \n",
    "cw_overdue = tracker_12mo_dataset_df[tracker_12mo_dataset_df['release_status']=='Scheduled for release'].shape[0]\n",
    "\n",
    "citywide = pd.DataFrame([['Citywide',\n",
    "                         cw_numrows,\n",
    "                         cw_numdatasets,\n",
    "                         cw_freshness,\n",
    "                         cw_compliance,\n",
    "                         cw_overdue]],\n",
    "                       columns=['Scope',\n",
    "                                'Number of published rows',\n",
    "                                'Number of published datasets',\n",
    "                                'Percent of datasets updated on time',\n",
    "                                'Percent of datasets released on time in the last 12 months',\n",
    "                                'Number of overdue datasets'])\n",
    "\n",
    "citywide = citywide.fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2. Build complete agency-level dataset\n",
    "\n",
    "all_agency_df = quantity_agency_df.merge(freshness_agency_df, \n",
    "                                        on='agency',\n",
    "                                        how='outer')\\\n",
    "                                  .merge(tracker_12mo_agency_df, \n",
    "                                        on='agency',\n",
    "                                        how='outer')\n",
    "\n",
    "# all_agency_df = all_agency_df.fillna('NA')\n",
    "all_agency_df['overdue_datasets'] = all_agency_df['overdue_datasets'].fillna(0)\n",
    "all_agency_df['numdatasets'] = all_agency_df['numdatasets'].fillna(0)\n",
    "all_agency_df['numrows'] = all_agency_df['numrows'].fillna(0)\n",
    "all_agency_df['total_auto_count'] = all_agency_df['total_auto_count'].fillna(0)\n",
    "all_agency_df['fresh_count'] = all_agency_df['fresh_count'].fillna(0)\n",
    "all_agency_df['tracker_dataset_count'] = all_agency_df['tracker_dataset_count'].fillna(0)\n",
    "all_agency_df['tracker_count_ontime'] = all_agency_df['tracker_count_ontime'].fillna(0)\n",
    "all_agency_df['fresh_pct'] = all_agency_df['fresh_pct'].fillna('No automated datasets')\n",
    "all_agency_df['pct_ontime'] = all_agency_df['pct_ontime'].fillna('No datasets in the tracker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 3. Build complete dataset-level dataset\n",
    "\n",
    "# aggregate freshness data and tracker data (for released datasets only)\n",
    "all_datasets_df = quantity_dataset_df.merge(freshness_dataset_df.drop(columns=\n",
    "                                                                      ['agency',\n",
    "                                                                      'name',\n",
    "                                                                      'dataset_link',\n",
    "                                                                      'last_update_date_data_dt']), \n",
    "                                        on='u_id',\n",
    "                                        how='outer')\\\n",
    "                                  .merge(tracker_12mo_dataset_df.drop(columns=\n",
    "                                                                            ['agency',\n",
    "                                                                             'dataset',\n",
    "                                                                             'type',\n",
    "                                                                             'dataset_link']), \n",
    "                                        on='u_id',\n",
    "                                        how='left')\n",
    "\n",
    "# append non-released datasets data\n",
    "# doing it as a separate step to keep more accurate data for released datasets\n",
    "all_datasets_df = all_datasets_df.append(tracker_12mo_dataset_df[~tracker_12mo_dataset_df.u_id.isin(all_datasets_df.u_id)])\\\n",
    "                                 .reset_index(drop=True)\n",
    "\n",
    "# merge name and dataset columns since they contain the same information\n",
    "all_datasets_df.loc[all_datasets_df.name.isna(),'name'] = all_datasets_df.dataset\n",
    "\n",
    "# merge automation/update data for \"historical\" and \"as needed\" datasets\n",
    "all_datasets_df = all_datasets_df.merge(public_df[['u_id','automation','update_frequency']], on='u_id', how='left')\n",
    "all_datasets_df['automation'] = all_datasets_df.automation_x\n",
    "all_datasets_df['update_frequency'] = all_datasets_df.update_frequency_x\n",
    "all_datasets_df.loc[all_datasets_df.automation_x.isna(),'automation'] = all_datasets_df.automation_y\n",
    "all_datasets_df.loc[all_datasets_df.update_frequency_x.isna(),'update_frequency'] = all_datasets_df.update_frequency_y\n",
    "\n",
    "# recode missing dates into string NA to properly read format in GDS\n",
    "all_datasets_df['release_date_dt_fix'] = pd.to_datetime(all_datasets_df.release_date_dt, errors='coerce')\n",
    "\n",
    "all_datasets_df = all_datasets_df[[\n",
    " 'agency',\n",
    " 'u_id',\n",
    " 'name',\n",
    " 'dataset_description',\n",
    " 'dataset_link',\n",
    " 'type',\n",
    " 'date_made_public_dt',\n",
    " 'numrows',\n",
    " 'automation',\n",
    " 'update_frequency',\n",
    " 'last_update_date_data_dt',\n",
    " 'fresh',\n",
    " 'latest_plan_date_dt',\n",
    " 'release_status',\n",
    " 'release_date_dt_fix',\n",
    " 'within_grace_period']]\n",
    "\n",
    "all_datasets_df['fresh'] = all_datasets_df['fresh'].fillna('No regular updates')\n",
    "all_datasets_df['within_grace_period'] = all_datasets_df['within_grace_period'].fillna('Not in Open Plan Tracker')\n",
    "# all_datasets_df = all_datasets_df.fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2440, 16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Step 4. Upload data to Google Spreadsheets\n",
    "\n",
    "# credentials.gs_upload(df=citywide, \n",
    "#                     wks_name='_citywide_')\n",
    "# print('Upload complete for citywide dataset')\n",
    "\n",
    "# credentials.gs_upload(df=all_agency_df, \n",
    "#                     wks_name='_agency_')\n",
    "# print('Upload complete for agency dataset')\n",
    "\n",
    "# credentials.gs_upload(df=all_datasets_df, \n",
    "#                     wks_name='_datasets_')\n",
    "# print('Upload complete for datasets dataset')\n",
    "\n",
    "# credentials.gs_upload(df=dates_df, \n",
    "#                     wks_name='_dates_')\n",
    "# print('Upload complete for dates dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard was updated on: 2020-09-28\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dashboard was updated on: {date.today()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scope</th>\n",
       "      <th>Number of published rows</th>\n",
       "      <th>Number of published datasets</th>\n",
       "      <th>Percent of datasets updated on time</th>\n",
       "      <th>Percent of datasets released on time in the last 12 months</th>\n",
       "      <th>Number of overdue datasets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Citywide</td>\n",
       "      <td>2.603060e+09</td>\n",
       "      <td>2440</td>\n",
       "      <td>0.403689</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scope  Number of published rows  Number of published datasets  \\\n",
       "0  Citywide              2.603060e+09                          2440   \n",
       "\n",
       "   Percent of datasets updated on time  \\\n",
       "0                             0.403689   \n",
       "\n",
       "   Percent of datasets released on time in the last 12 months  \\\n",
       "0                                           0.794118            \n",
       "\n",
       "   Number of overdue datasets  \n",
       "0                           0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citywide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
